# ğŸ¤Ÿ Tradutor de LIBRAS â€“ ComunicaÃ§Ã£o Inclusiva em Tempo Real

Este Ã© um aplicativo mobile criado para facilitar a **comunicaÃ§Ã£o entre pessoas surdas/mudas e ouvintes**, traduzindo sinais em **LIBRAS (LÃ­ngua Brasileira de Sinais)** em **texto e Ã¡udio em tempo real**, usando a cÃ¢mera do dispositivo, visÃ£o computacional e inteligÃªncia artificial.

---

## ğŸ“± Funcionalidades

- ğŸ“¸ Captura de sinais com a cÃ¢mera do celular
- âœ‹ Reconhecimento de gestos em LIBRAS (letras, palavras e frases)
- ğŸ§  ConversÃ£o dos sinais em texto usando IA treinada
- ğŸ”Š TransformaÃ§Ã£o do texto em fala (TTS) para comunicaÃ§Ã£o com ouvintes
- âš™ï¸ ConfiguraÃ§Ãµes de voz, idioma e velocidade da fala
- ğŸ”’ Interface acessÃ­vel, leve e otimizada para dispositivos mÃ³veis

---
## ğŸš€ Como rodar o projeto
- PrÃ© requisitos 
 - Node.js
 - Expo CLI
 - VSCode
 - App Expo Go no celular

 ### InstalaÃ§Ã£o do app mobile
 ```bash
 git clone https://github.com/seu-usuario/libras-translator-app.git
 cd libras-translator-app/mobile
 npm install
 npm start 
 ```
 - Use o QR Code no Expo Go para rodar no celular.
---

## ğŸ§  Arquitetura do Projeto

```text
UsuÃ¡rio faz sinal com a mÃ£o
        â†“
Camera do celular com MediaPipe Hands
        â†“
Detecta posiÃ§Ã£o e movimento dos dedos
        â†“
Modelo de IA interpreta o gesto â†’ Palavra
        â†“
Palavra aparece como TEXTO na tela
        â†“
Text-to-Speech (TTS) converte para Ã¡udio
        â†“
Ãudio Ã© reproduzido para pessoa ouvinte 
```

## ğŸ—‚ï¸ Estrutura de Pastas

```text
libras-translator-app/
â”œâ”€â”€ mobile/                  # App em React Native
â”‚   â”œâ”€â”€ App.js
â”‚   â”œâ”€â”€ screens/             # Telas: Home, TraduÃ§Ã£o, Config
â”‚   â”œâ”€â”€ components/          # Componentes reutilizÃ¡veis
â”‚   â”œâ”€â”€ services/            # IntegraÃ§Ã£o com TTS e API
â”‚   â”œâ”€â”€ assets/              # Ãcones, imagens
â”‚   â””â”€â”€ utils/               # FunÃ§Ãµes auxiliares
â”‚
â”œâ”€â”€ backend-ia/              # API Python para reconhecimento de gestos
â”‚   â”œâ”€â”€ main.py              # Endpoints de traduÃ§Ã£o
â”‚   â”œâ”€â”€ models/              # Modelos treinados
â”‚   â”œâ”€â”€ preprocessing/       # PrÃ©-processamento dos dados
â”‚   â””â”€â”€ media/               # Dados brutos para treino/teste
â”‚
â”œâ”€â”€ datasets/                # Base de dados de sinais
â””â”€â”€ README.md
```
---
## âš™ï¸ Tecnologias Utilizadas

### ğŸ“± Mobile
- [React Native](https://reactnative.dev/)
- [Expo](https://expo.dev/)
- [React Navigation](https://reactnavigation.org/)
- [expo-speech](https://docs.expo.dev/versions/latest/sdk/speech/)

### ğŸ§  IA e Backend
- [MediaPipe](https://google.github.io/mediapipe/)
- [OpenCV](https://opencv.org/)
- [TensorFlow](https://www.tensorflow.org/)
- [Flask](https://flask.palletsprojects.com/) ou [FastAPI](https://fastapi.tiangolo.com/)
- [Google Text-to-Speech API](https://cloud.google.com/text-to-speech)
---